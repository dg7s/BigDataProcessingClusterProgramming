{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqj2mn8MEOvB"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.functions import udf, col, lit, explode, collect_list\n",
        "from pyspark.sql.types import *\n",
        "import hashlib\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import itertools\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q assignment_data.zip -d assignment_data\n",
        "\n",
        "%cd assignment_data\n",
        "\n",
        "!find . -type f \\( -name \"_SUCCESS\" -o -name \"*.crc\" -o -not -name \"*.csv\" \\) -delete\n",
        "\n",
        "print(\"\\nFinal files:\")\n",
        "!find . -type f -name \"*.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOHI6WCshFw2",
        "outputId": "2fe30b2d-4658-42b9-9cbc-58bc0c56973e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/dg7/assignment_data\n",
            "\n",
            "Zawartość po oczyszczeniu:\n",
            "./assignment_gold_sample.csv/part-00000-092b60af-3a90-468c-9e7b-07a5cfb40802-c000.csv\n",
            "./assignment_questions.csv/part-00012-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00009-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00001-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00011-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00000-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00008-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00003-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00010-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00004-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00005-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00007-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00006-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "./assignment_questions.csv/part-00002-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mkdir -p /data/assignment_questions\n",
        "!hdfs dfs -mkdir -p /data/gold_sample\n",
        "\n",
        "!hdfs dfs -put ./assignment_questions.csv/*.csv /data/assignment_questions/\n",
        "!hdfs dfs -put ./assignment_gold_sample.csv/*.csv /data/gold_sample/"
      ],
      "metadata": {
        "id": "UsUPiPh1h7i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Questions files at HDFS:\")\n",
        "!hdfs dfs -ls /data/assignment_questions\n",
        "\n",
        "print(\"\\nGold sample at HDFS:\")\n",
        "!hdfs dfs -ls /data/gold_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTaUr0sUiE5r",
        "outputId": "4b40f2c0-8eec-4932-e59e-552c9331bf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions files at HDFS:\n",
            "Found 13 items\n",
            "-rw-r--r--   3 dg7 supergroup    2995447 2025-04-21 22:46 /data/assignment_questions/part-00000-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3021293 2025-04-21 22:46 /data/assignment_questions/part-00001-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3033133 2025-04-21 22:46 /data/assignment_questions/part-00002-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    2998842 2025-04-21 22:46 /data/assignment_questions/part-00003-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3013925 2025-04-21 22:46 /data/assignment_questions/part-00004-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3009866 2025-04-21 22:46 /data/assignment_questions/part-00005-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    2994744 2025-04-21 22:46 /data/assignment_questions/part-00006-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3005591 2025-04-21 22:46 /data/assignment_questions/part-00007-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3027741 2025-04-21 22:46 /data/assignment_questions/part-00008-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3021923 2025-04-21 22:46 /data/assignment_questions/part-00009-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3028537 2025-04-21 22:46 /data/assignment_questions/part-00010-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    3001660 2025-04-21 22:46 /data/assignment_questions/part-00011-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "-rw-r--r--   3 dg7 supergroup    1493972 2025-04-21 22:46 /data/assignment_questions/part-00012-d081a743-261e-4834-b802-e980be0f64b1-c000.csv\n",
            "\n",
            "Gold sample at HDFS:\n",
            "Found 1 items\n",
            "-rw-r--r--   3 dg7 supergroup    6177387 2025-04-21 22:46 /data/gold_sample/part-00000-092b60af-3a90-468c-9e7b-07a5cfb40802-c000.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "25/05/13 16:41:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "                    .master(\"spark://master:7077\") \\\n",
        "                    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "                    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "                    .appName(\"LSH\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "print(f'The PySpark {spark.version} version is running...')"
      ],
      "metadata": {
        "id": "pwHpBmg0H5QU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c8d79c-14c1-4b31-eef5-a2cc0040edc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The PySpark 3.5.5 version is running...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cluster consists of one master node and two worker nodes, as well as a driver. Each of these is an e2-medium virtual machine, equipped with 2 vCPUs and 4 GB of memory."
      ],
      "metadata": {
        "id": "2ppOQFFoZeLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), nullable=False),\n",
        "    StructField(\"question\", StringType(), nullable=False)\n",
        "])\n",
        "\n",
        "gold_schema = StructType([\n",
        "    StructField(\"qid1\", IntegerType(), nullable=False),\n",
        "    StructField(\"qid2\", IntegerType(), nullable=False),\n",
        "    StructField(\"is_duplicate\", IntegerType(), nullable=False)\n",
        "])\n"
      ],
      "metadata": {
        "id": "zMIpIL-VjDAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_partitioned_data(base_path):\n",
        "    return spark.read \\\n",
        "        .schema(questions_schema if \"questions\" in base_path else gold_schema) \\\n",
        "        .csv(f\"{base_path}/*.csv\", header=True)"
      ],
      "metadata": {
        "id": "hDcanxrFjMNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_path = \"hdfs://master:9000/data/assignment_questions\"\n",
        "gold_sample_path = \"hdfs://master:9000/data/gold_sample\""
      ],
      "metadata": {
        "id": "fROKrIMAjSeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_df = load_partitioned_data(questions_path)\n",
        "gold_df = load_partitioned_data(gold_sample_path)\n",
        "\n",
        "print(\"\\n=== Statistics ===\")\n",
        "print(f\"Number of questions: {questions_df.count()}\")\n",
        "print(f\"Number of gold pairs: {gold_df.count()}\")\n",
        "\n",
        "print(\"\\n=== Sample questions ===\")\n",
        "questions_df.show(5, truncate=50)\n",
        "\n",
        "print(\"\\n=== Sample gold pairs ===\")\n",
        "gold_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnbTUGrdjZU6",
        "outputId": "f0945d7a-6467-434d-eec6-9df97b701412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Statistics ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of questions: 537933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of gold pairs: 404288\n",
            "\n",
            "=== Sample questions ===\n",
            "+----+--------------------------------------------------+\n",
            "|  id|                                          question|\n",
            "+----+--------------------------------------------------+\n",
            "| 221|Where can I watch gonulcelen with english subti...|\n",
            "|1080|Is it necessary to unlock bootloader before roo...|\n",
            "|1729|             How can constipation cause dizziness?|\n",
            "|2454|How should I get motivated to hit the gym every...|\n",
            "|3047|   How long will the Pokémon GO \"fever\" will last?|\n",
            "+----+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "=== Sample gold pairs ===\n",
            "+----+----+------------+\n",
            "|qid1|qid2|is_duplicate|\n",
            "+----+----+------------+\n",
            "|   1|   2|           0|\n",
            "|   3|   4|           0|\n",
            "|   5|   6|           0|\n",
            "|   7|   8|           0|\n",
            "|   9|  10|           0|\n",
            "+----+----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_data(df, columns):\n",
        "    for column in columns:\n",
        "        null_count = df.filter(col(column).isNull()).count()\n",
        "        print(f\"Missing values in {column}: {null_count}\")\n",
        "\n",
        "print(\"\\n=== Questions validation ===\")\n",
        "validate_data(questions_df, [\"id\", \"question\"])\n",
        "\n",
        "print(\"\\n=== Gold sample validation ===\")\n",
        "validate_data(gold_df, [\"qid1\", \"qid2\", \"is_duplicate\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjM9VV1wlruE",
        "outputId": "2d9a0797-454a-48bf-9405-bddea9650608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Questions validation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in id: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in question: 1\n",
            "\n",
            "=== Gold sample validation ===\n",
            "Missing values in qid1: 0\n",
            "Missing values in qid2: 0\n",
            "Missing values in is_duplicate: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set `MAX_MOD = sys.maxsize` to ensure that all our hash values fit within a 64-bit signed integer (`LongType` in Spark). This keeps the shingle hashes and MinHash signatures in a consistent, efficient numeric range without risking overflow or loss of precision."
      ],
      "metadata": {
        "id": "o9v6GOy81hLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALLOWED_CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789 .,!?\\\"'\"\n",
        "CHAR_TO_INT = {c: i+1 for i, c in enumerate(ALLOWED_CHARS)}\n",
        "BASE = len(ALLOWED_CHARS) + 1  # 39 + 1 = 40\n",
        "MAX_MOD = sys.maxsize  # 9,223,372,036,854,775,807 dla Int64\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    return re.sub(r\"[^{}]\".format(re.escape(ALLOWED_CHARS)), \"\", text.lower()).strip()\n",
        "\n",
        "\n",
        "@udf(ArrayType(LongType()))\n",
        "def shingle_hash(text: str, k: int = 3) -> list:\n",
        "    text_clean = preprocess(text)\n",
        "    n = len(text_clean)\n",
        "    shingles = []\n",
        "\n",
        "    if n < k:\n",
        "        return []\n",
        "\n",
        "    powers = [BASE ** i for i in range(k-1, -1, -1)]\n",
        "\n",
        "    for i in range(n - k + 1):\n",
        "        h = sum(CHAR_TO_INT.get(c,0)*powers[j] for j,c in enumerate(text_clean[i:i+k])) % MAX_MOD\n",
        "        shingles.append(h)\n",
        "\n",
        "    return list(set(shingles))"
      ],
      "metadata": {
        "id": "AK89VphosLhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tested both SHA-256-based hashing and simpler linear hash functions to compare their speed and accuracy in our MinHash/LSH pipeline.\n",
        "\n",
        "With **k=3, sig_size=50, bands=5**:\n",
        "- **SHA-256** took **50 min** (TP=9608, FP=7212)  \n",
        "- **Linear hash** took **14 min** (TP=10115, \tFP=7418)  \n",
        "\n",
        "Because of the speedup and similar TP rate we stick with the linear-hash method."
      ],
      "metadata": {
        "id": "wsLg_g6Q1q8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_hash_functions(num_hashes: int, mod: int = MAX_MOD):\n",
        "    \"\"\"\n",
        "    h_i(x) = SHA256(i || x) % mod.\n",
        "    \"\"\"\n",
        "    funcs = []\n",
        "    for i in range(num_hashes):\n",
        "        def _fn(x, i=i, _mod=mod):\n",
        "            h = hashlib.sha256(f\"minhash_{i}_{x}\".encode()).hexdigest()\n",
        "            return int(h, 16) % _mod\n",
        "        funcs.append(_fn)\n",
        "    return funcs\n",
        "\n",
        "def make_simple_hash_functions(num_hashes: int, mod: int = MAX_MOD):\n",
        "    \"\"\"\n",
        "    h_i(x) = shingle * random_number_1 + random_number_2.\n",
        "    \"\"\"\n",
        "    funcs = []\n",
        "    for i in range(num_hashes):\n",
        "        random_number_1 = random.randint(1, mod)\n",
        "        random_number_2 = random.randint(0, mod)\n",
        "\n",
        "        def _fn(shingle, _random_number_1=random_number_1, _random_number_2=random_number_2, _mod=mod):\n",
        "            if not shingle:\n",
        "                return 0\n",
        "            return (shingle * _random_number_1 + _random_number_2) % _mod\n",
        "\n",
        "        funcs.append(_fn)\n",
        "\n",
        "    return funcs\n",
        "\n",
        "\n",
        "\n",
        "def make_minhash_signature_udf(hash_funcs):\n",
        "    @udf(ArrayType(LongType()))\n",
        "    def minhash_signature(shingles: list) -> list:\n",
        "        if not shingles:\n",
        "            return [0] * len(hash_funcs)\n",
        "        return [min(fn(s) for s in shingles) for fn in hash_funcs]\n",
        "    return minhash_signature\n",
        "\n",
        "def make_lsh_bands_udf(band_funcs):\n",
        "    \"\"\"\n",
        "    band_funcs: function list: [h_0, h_1, ..., h_{B-1}],\n",
        "    h_i(x) = x*a_i + b_i % mod\n",
        "    \"\"\"\n",
        "    @udf(ArrayType(StringType()))\n",
        "    def lsh_bands(signature: list) -> list:\n",
        "        if not signature:\n",
        "            return []\n",
        "        B = len(band_funcs)\n",
        "        r = len(signature) // B\n",
        "        out = []\n",
        "        for i in range(B):\n",
        "            chunk = signature[i*r:(i+1)*r]\n",
        "            # we sum the chunk to one number\n",
        "            x = sum(chunk)\n",
        "            h = band_funcs[i](x)\n",
        "            out.append(f\"{i}:{h}\")\n",
        "        return out\n",
        "    return lsh_bands"
      ],
      "metadata": {
        "id": "jyTf4hb7TMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our 2 workers × 2 vCPUs give 4 cores total. Using 8 partitions ensures each core handles two tasks, preventing slow stragglers.  "
      ],
      "metadata": {
        "id": "0QakIz8p2IXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_partitions = 8\n",
        "questions = spark.read.csv(\"hdfs://master:9000/data/assignment_questions\", schema=questions_schema)\n",
        "questions = questions.cache()\n",
        "\n",
        "gold = spark.read.csv(\"hdfs://master:9000/data/gold_sample\", schema=gold_schema)\n",
        "\n",
        "questions=questions.filter(col(\"id\").isNotNull()).repartition(num_partitions)\n",
        "gold=gold.filter(col(\"qid1\").isNotNull() & col(\"qid2\").isNotNull()).cache()\n",
        "\n",
        "PARAM_COMBINATIONS = [\n",
        "    (4, 60, 6),\n",
        "    (3, 70, 7),\n",
        "    (5, 50, 10),\n",
        "    (4, 80, 8),\n",
        "    (5, 100, 10),\n",
        "    (9, 60, 6),\n",
        "    (5, 30, 10),\n",
        "    (3, 50, 5)\n",
        "]"
      ],
      "metadata": {
        "id": "QNYqoKXlE_51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3feb387a-c4f7-4f22-a502-da87b3146def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "25/05/13 17:37:43 WARN CacheManager: Asked to cache already cached data.\n",
            "25/05/13 17:37:43 WARN CacheManager: Asked to cache already cached data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(params):\n",
        "    k, signature_size, num_bands = params\n",
        "    try:\n",
        "        hash_funcs = make_simple_hash_functions(signature_size, MAX_MOD)\n",
        "        minhash_udf = make_minhash_signature_udf(hash_funcs)\n",
        "\n",
        "        band_funcs      = make_simple_hash_functions(num_bands, MAX_MOD)\n",
        "        lsh_bands_udf   = make_lsh_bands_udf(band_funcs)\n",
        "\n",
        "        df = (\n",
        "            questions\n",
        "            .withColumn(\"shingles\", shingle_hash(col(\"question\"), lit(k)))\n",
        "            .withColumn(\"signature\", minhash_udf(col(\"shingles\")))\n",
        "            .withColumn(\"band\", explode(lsh_bands_udf(col(\"signature\"))))\n",
        "        )\n",
        "\n",
        "        buckets = df.select(\"id\", \"band\").groupBy(\"band\").agg(collect_list(\"id\").alias(\"ids\"))\n",
        "        pairs = buckets.rdd.flatMap(lambda row: itertools.combinations(sorted(row.ids), 2)).distinct()\n",
        "\n",
        "        # Evaluate TP/FP\n",
        "        gold_pairs = gold.rdd.map(lambda x: ((x.qid1, x.qid2), x.is_duplicate))\n",
        "        results = pairs.map(lambda p: (p, 1)).leftOuterJoin(gold_pairs)\n",
        "        tp = results.filter(lambda x: x[1][1] == 1).count()\n",
        "        fp = results.filter(lambda x: x[1][1] == 0).count()\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "        print(f\"Results: k={k}, sig_size={signature_size}, bands={num_bands}\")\n",
        "\n",
        "        return {\n",
        "            \"k\": k,\n",
        "            \"signature_size\": signature_size,\n",
        "            \"num_bands\": num_bands,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"precision\": precision\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error for params {params}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "qVI2O8HNgTKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV      = \"lsh_results.csv\"\n",
        "HDFS_DIR = \"hdfs://master:9000/results/lsh_experiments\"\n",
        "\n",
        "# Initialize local CSV file with header if it doesn't exist yet\n",
        "fieldnames = ['k','signature_size','num_bands','tp','fp','precision']\n",
        "if not os.path.exists(CSV):\n",
        "    with open(CSV, 'w', newline='') as f:\n",
        "        csv.writer(f).writerow(fieldnames)\n",
        "\n",
        "# Load already processed parameter combinations from the CSV\n",
        "seen = set()\n",
        "with open(CSV, newline='') as f:\n",
        "    rd = csv.DictReader(f)\n",
        "    for r in rd:\n",
        "        seen.add((int(r['k']), int(r['signature_size']), int(r['num_bands'])))\n",
        "\n",
        "# Initialize empty Parquet file on HDFS for appending experiment results\n",
        "schema = \"k int, signature_size int, num_bands int, tp int, fp int, precision double\"\n",
        "spark.createDataFrame([], schema).write.mode(\"overwrite\").parquet(HDFS_DIR)\n",
        "\n",
        "# Run loop\n",
        "for k, signature_size, num_bands in PARAM_COMBINATIONS:\n",
        "    if (k, signature_size, num_bands) in seen:\n",
        "        print(f\"Skipping params {(k, signature_size, num_bands)} – already computed\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Starting experiment for params: k={k}, signature_size={signature_size}, num_bands={num_bands}\")\n",
        "    res = run_experiment((k, signature_size, num_bands))\n",
        "    if not isinstance(res, dict):\n",
        "        print(f\"Unexpected result for {(k, signature_size, num_bands)}, skipping\")\n",
        "        continue\n",
        "\n",
        "    # Format the result as a row for CSV and Parquet\n",
        "    row = [res['k'], res['signature_size'], res['num_bands'], res['tp'], res['fp'], res['precision']]\n",
        "\n",
        "    # write to CSV\n",
        "    with open(CSV, 'a', newline='') as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "    # write to HDFS\n",
        "    df = spark.createDataFrame([tuple(row)], fieldnames)\n",
        "    df.write.mode(\"append\").parquet(HDFS_DIR)\n",
        "\n",
        "    print(f\"Finished: {(k, signature_size, num_bands)} → tp={res['tp']}, fp={res['fp']}, precision={res['precision']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF-Kdv8LacA6",
        "outputId": "8daef773-13be-4999-e692-abe25b9cf3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping params (4, 60, 6) – already computed\n",
            "Skipping params (3, 70, 7) – already computed\n",
            "Skipping params (5, 50, 10) – already computed\n",
            "Skipping params (4, 80, 8) – already computed\n",
            "Skipping params (5, 100, 10) – already computed\n",
            "Skipping params (9, 60, 6) – already computed\n",
            "Skipping params (5, 30, 10) – already computed\n",
            "Starting experiment for params: k=3, signature_size=50, num_bands=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: k=3, sig_size=50, bands=5\n",
            "Finished: (3, 50, 5) → tp=10115, fp=7418, precision=0.5769121085952205\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df = pd.read_csv(\"lsh_results.csv\")\n",
        "df = df.round(3)\n",
        "df = df.sort_values(by=[\"precision\"], ascending=False)\n",
        "display(df.style.hide(axis=\"index\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Y11J8qaBhBNQ",
        "outputId": "65c33821-b433-4c75-88c4-719be953454e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f25a6ec9050>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_6dc27\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_6dc27_level0_col0\" class=\"col_heading level0 col0\" >k</th>\n",
              "      <th id=\"T_6dc27_level0_col1\" class=\"col_heading level0 col1\" >signature_size</th>\n",
              "      <th id=\"T_6dc27_level0_col2\" class=\"col_heading level0 col2\" >num_bands</th>\n",
              "      <th id=\"T_6dc27_level0_col3\" class=\"col_heading level0 col3\" >tp</th>\n",
              "      <th id=\"T_6dc27_level0_col4\" class=\"col_heading level0 col4\" >fp</th>\n",
              "      <th id=\"T_6dc27_level0_col5\" class=\"col_heading level0 col5\" >precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row0_col0\" class=\"data row0 col0\" >3</td>\n",
              "      <td id=\"T_6dc27_row0_col1\" class=\"data row0 col1\" >50</td>\n",
              "      <td id=\"T_6dc27_row0_col2\" class=\"data row0 col2\" >5</td>\n",
              "      <td id=\"T_6dc27_row0_col3\" class=\"data row0 col3\" >10115</td>\n",
              "      <td id=\"T_6dc27_row0_col4\" class=\"data row0 col4\" >7418</td>\n",
              "      <td id=\"T_6dc27_row0_col5\" class=\"data row0 col5\" >0.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "      <td id=\"T_6dc27_row1_col1\" class=\"data row1 col1\" >70</td>\n",
              "      <td id=\"T_6dc27_row1_col2\" class=\"data row1 col2\" >7</td>\n",
              "      <td id=\"T_6dc27_row1_col3\" class=\"data row1 col3\" >11842</td>\n",
              "      <td id=\"T_6dc27_row1_col4\" class=\"data row1 col4\" >9103</td>\n",
              "      <td id=\"T_6dc27_row1_col5\" class=\"data row1 col5\" >0.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "      <td id=\"T_6dc27_row2_col1\" class=\"data row2 col1\" >60</td>\n",
              "      <td id=\"T_6dc27_row2_col2\" class=\"data row2 col2\" >6</td>\n",
              "      <td id=\"T_6dc27_row2_col3\" class=\"data row2 col3\" >7844</td>\n",
              "      <td id=\"T_6dc27_row2_col4\" class=\"data row2 col4\" >6718</td>\n",
              "      <td id=\"T_6dc27_row2_col5\" class=\"data row2 col5\" >0.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_6dc27_row3_col1\" class=\"data row3 col1\" >80</td>\n",
              "      <td id=\"T_6dc27_row3_col2\" class=\"data row3 col2\" >8</td>\n",
              "      <td id=\"T_6dc27_row3_col3\" class=\"data row3 col3\" >8887</td>\n",
              "      <td id=\"T_6dc27_row3_col4\" class=\"data row3 col4\" >7867</td>\n",
              "      <td id=\"T_6dc27_row3_col5\" class=\"data row3 col5\" >0.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row4_col0\" class=\"data row4 col0\" >5</td>\n",
              "      <td id=\"T_6dc27_row4_col1\" class=\"data row4 col1\" >50</td>\n",
              "      <td id=\"T_6dc27_row4_col2\" class=\"data row4 col2\" >10</td>\n",
              "      <td id=\"T_6dc27_row4_col3\" class=\"data row4 col3\" >21509</td>\n",
              "      <td id=\"T_6dc27_row4_col4\" class=\"data row4 col4\" >19879</td>\n",
              "      <td id=\"T_6dc27_row4_col5\" class=\"data row4 col5\" >0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row5_col0\" class=\"data row5 col0\" >5</td>\n",
              "      <td id=\"T_6dc27_row5_col1\" class=\"data row5 col1\" >30</td>\n",
              "      <td id=\"T_6dc27_row5_col2\" class=\"data row5 col2\" >10</td>\n",
              "      <td id=\"T_6dc27_row5_col3\" class=\"data row5 col3\" >43093</td>\n",
              "      <td id=\"T_6dc27_row5_col4\" class=\"data row5 col4\" >40344</td>\n",
              "      <td id=\"T_6dc27_row5_col5\" class=\"data row5 col5\" >0.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row6_col0\" class=\"data row6 col0\" >5</td>\n",
              "      <td id=\"T_6dc27_row6_col1\" class=\"data row6 col1\" >100</td>\n",
              "      <td id=\"T_6dc27_row6_col2\" class=\"data row6 col2\" >10</td>\n",
              "      <td id=\"T_6dc27_row6_col3\" class=\"data row6 col3\" >7380</td>\n",
              "      <td id=\"T_6dc27_row6_col4\" class=\"data row6 col4\" >7351</td>\n",
              "      <td id=\"T_6dc27_row6_col5\" class=\"data row6 col5\" >0.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_6dc27_row7_col0\" class=\"data row7 col0\" >9</td>\n",
              "      <td id=\"T_6dc27_row7_col1\" class=\"data row7 col1\" >60</td>\n",
              "      <td id=\"T_6dc27_row7_col2\" class=\"data row7 col2\" >6</td>\n",
              "      <td id=\"T_6dc27_row7_col3\" class=\"data row7 col3\" >2751</td>\n",
              "      <td id=\"T_6dc27_row7_col4\" class=\"data row7 col4\" >3172</td>\n",
              "      <td id=\"T_6dc27_row7_col5\" class=\"data row7 col5\" >0.464000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "QIat70ow77yC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}