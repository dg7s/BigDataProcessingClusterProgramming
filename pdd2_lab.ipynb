{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeRy_Ng0lfDT"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>\n",
        "\n",
        "**Author: Jacek Sroka**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDD - LAB 2"
      ],
      "metadata": {
        "id": "0uEERM15Z3i6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan\n",
        "\n",
        "1. Installation\n",
        "2. Introduction to RDD API\n",
        "3. Broadcast variables\n",
        "4. Homework\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao9tweXeZr_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To **edit the colab** first copy it to your drive with the top bars `File -> Save a copy on drive` option."
      ],
      "metadata": {
        "id": "w5XTROAjE7ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n"
      ],
      "metadata": {
        "id": "qdrOqKrbbWEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxqy7fQ2Zn_u"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -U -q PyDrive --quiet\n",
        "!apt install openjdk-8-jdk-headless &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "id": "hvP2cvhBb-gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"mlibs\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "metadata": {
        "id": "IRX13sVIcU9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: pyspark documentation is [here](https://spark.apache.org/docs/3.1.2/api/python/reference/index.html). For example: list of `SparkContext` methods is [here](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.html#spark-context-apis).\n"
      ],
      "metadata": {
        "id": "fC4mnbWTYtkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to RDD API\n",
        "We start with a sample RDD obtained from a local Python collection. The second parameter of `parallelize` specifies how many partitions we want to have and thus allows us to control the parallelism. Usually, it is good to have 4x more partitions than cores, but having partitions with too few elements can make the overhead noticeable. On the other hand for some algorithms, the rate of data transfer over the network can grow quickly with the number of partitions (for example the algorithm could operate on partition pairs).\n",
        "\n",
        "`cache` suggests to Spark to keep this RDD in cluster memory to speed up future usage."
      ],
      "metadata": {
        "id": "XUtLR5z2dUNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_rdd = sc.parallelize(range(1, 10), 2).cache()"
      ],
      "metadata": {
        "id": "G_NdrZeQNO5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`collect` converts the rdd into local python collection on the driver side."
      ],
      "metadata": {
        "id": "zm-geH4uYNLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_rdd.collect()"
      ],
      "metadata": {
        "id": "yFYEZ4rCYDLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aef724b-2007-46e6-96da-1de7e86ac9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try actions: `first`, `count`, `take(n)` and `takeSample(withReplacement, num, [seed])`. The documentation can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)."
      ],
      "metadata": {
        "id": "iSqNOasqW7ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "print(input_rdd.first())\n",
        "print(input_rdd.count())\n",
        "print(input_rdd.take(3))\n",
        "print(input_rdd.takeSample(True, 6))"
      ],
      "metadata": {
        "id": "ektM1eJWg3A6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64e5722-549e-4277-a2d3-c624c5c3df24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "9\n",
            "[1, 2, 3]\n",
            "[7, 1, 6, 6, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the total of the RDD elements with `reduce(func)`.\n",
        "\n",
        "RDD API with all the actions and transformations can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD)."
      ],
      "metadata": {
        "id": "JwIK_z-B8YVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put you code here\n",
        "input_rdd.reduce(lambda x, y: x+y)"
      ],
      "metadata": {
        "id": "QEgBWIsk87vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd5c4c4-4ece-4b5d-8562-c66ab6b0a6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs are divided into partitions and distributed on the cluster. We can check how RDD is obtained (lineage), how many partitions there are, and how much memory they use with the `toDebugString` method. PS. `decode(\"ascii\")` part is needed to print in colab only."
      ],
      "metadata": {
        "id": "r1WUeAj-XS43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_rdd.toDebugString().decode(\"ascii\"))"
      ],
      "metadata": {
        "id": "YICgWJTUWw3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783743db-4fdf-40f3-d7dc-c2e0cac39b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[1] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
            " |       CachedPartitions: 2; MemorySize: 176.0 B; DiskSize: 0.0 B\n",
            " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 [Memory Serialized 1x Replicated]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a new RDD by transforming each value of `input_rdd` with the `map` transformation."
      ],
      "metadata": {
        "id": "Dt9LSHutbA8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plus1 = input_rdd.map(lambda x: x+1)\n",
        "plus1.collect()"
      ],
      "metadata": {
        "id": "VPVic4F6W0Yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ffffb6-a5fd-40aa-bb79-48b15711f550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(plus1.toDebugString().decode(\"ascii\"))"
      ],
      "metadata": {
        "id": "EUIhIxksbrZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b4a789-35e6-4bde-e4eb-ef4a3b5e01ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[8] at collect at <ipython-input-9-8bfa8a4c60ee>:2 []\n",
            " |  PythonRDD[1] at RDD at PythonRDD.scala:53 []\n",
            " |      CachedPartitions: 2; MemorySize: 176.0 B; DiskSize: 0.0 B\n",
            " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try the `toDebugString` on the examples from lab1."
      ],
      "metadata": {
        "id": "XFgUHfxz-bBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create RDD from lines of the `sample_data/README.md` file and count the lines that contain the letter `a`. For that, you can use the `filter` transformation. Don't forget to cache the RDD as we will use it later."
      ],
      "metadata": {
        "id": "t0RObatYAnl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "lines_of_text = sc.textFile(\"sample_data/README.md\").cache()\n",
        "print(lines_of_text.filter(lambda line: 'a' in line).count())"
      ],
      "metadata": {
        "id": "5VkwFnXgb6tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3e833a-fef4-4ccc-f664-229d871dfb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the `lines_of_text` RDD to RDD of words by using `flatMap`, then use  `map` and `reduceByKey` to count the number of words in the MapReduce way. Note that this is similar to one of the exercises from lab 1, but here we expect you to use RDD API rather than the MapReduce API we provided last week."
      ],
      "metadata": {
        "id": "XU1EFUSQbI7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "words = lines_of_text.flatMap(lambda line: line.split())\n",
        "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x,y: x+y)\n",
        "word_counts.collect()"
      ],
      "metadata": {
        "id": "uez-OluTmn7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19629de-08ef-453d-f260-25baa9f8b273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sample', 2),\n",
              " ('datasets', 1),\n",
              " ('to', 1),\n",
              " ('you', 1),\n",
              " ('started.', 1),\n",
              " ('*', 3),\n",
              " ('`california_housing_data*.csv`', 1),\n",
              " ('from', 1),\n",
              " ('1990', 1),\n",
              " ('more', 1),\n",
              " ('information', 1),\n",
              " ('available', 1),\n",
              " ('https://docs.google.com/document/d/e/2PACX-1vRhYtsvc5eOR2FWNCwaBiKL6suIOrxJig8LcSBbmCbyYsayia_DvPOOBlXZ4CAlQ5nlDD8kTaIDRwrN/pub',\n",
              "  1),\n",
              " ('small', 1),\n",
              " ('of', 2),\n",
              " ('described', 2),\n",
              " ('quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);', 1),\n",
              " ('it', 1),\n",
              " ('Anscombe,', 1),\n",
              " ('F.', 1),\n",
              " ('J.', 1),\n",
              " (\"'Graphs\", 1),\n",
              " ('American', 1),\n",
              " ('Statistician.', 1),\n",
              " ('JSTOR', 1),\n",
              " ('2682899.', 1),\n",
              " ('and', 1),\n",
              " ('our', 1),\n",
              " ('by', 1),\n",
              " ('This', 1),\n",
              " ('directory', 1),\n",
              " ('includes', 1),\n",
              " ('a', 3),\n",
              " ('few', 1),\n",
              " ('get', 1),\n",
              " ('is', 4),\n",
              " ('California', 1),\n",
              " ('housing', 1),\n",
              " ('data', 1),\n",
              " ('the', 3),\n",
              " ('US', 1),\n",
              " ('Census;', 1),\n",
              " ('at:', 2),\n",
              " ('`mnist_*.csv`', 1),\n",
              " ('[MNIST', 1),\n",
              " ('database](https://en.wikipedia.org/wiki/MNIST_database),', 1),\n",
              " ('which', 1),\n",
              " ('http://yann.lecun.com/exdb/mnist/', 1),\n",
              " ('`anscombe.json`', 1),\n",
              " ('contains', 1),\n",
              " ('copy', 2),\n",
              " (\"[Anscombe's\", 1),\n",
              " ('was', 2),\n",
              " ('originally', 1),\n",
              " ('in', 2),\n",
              " ('(1973).', 1),\n",
              " ('Statistical', 1),\n",
              " (\"Analysis'.\", 1),\n",
              " ('27', 1),\n",
              " ('(1):', 1),\n",
              " ('17-21.', 1),\n",
              " ('prepared', 1),\n",
              " ('[vega_datasets', 1),\n",
              " ('library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).',\n",
              "  1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than processing large datasets, Spark can also distribute computation to many machines/cores. Use the Monte Carlo method to estimate the value of Pi. For that generate $n=4*10^5$ random points in square $[-1;1]\\times[-1;1]$ and count the proportion of the ones inside the largest circle contained by this square. Make sure that the computation can be distributed to at least 4 cores."
      ],
      "metadata": {
        "id": "z-b-JofOnds1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "\n",
        "slices = 4\n",
        "n = 100000 * slices\n",
        "\n",
        "xs = range(n)\n",
        "rdd = sc.parallelize(xs, slices).setName(\"'Initial rdd'\")\n",
        "sample = rdd.map(lambda i: (random.uniform(-1,1), random.uniform(-1,1))).setName(\"'Random points sample'\")\n",
        "inside_circle = sample.filter(lambda tup: tup[0]**2+tup[1]**2 <= 1).setName(\"'Random points inside circle'\")\n",
        "count = inside_circle.count()\n",
        "\n",
        "f\"Pi is roughly {4.0 * count / n}\""
      ],
      "metadata": {
        "id": "ysLuXFwOmDRk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce3f20b6-5604-440a-c0a7-82270de1f8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pi is roughly 3.1391'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcast variables\n",
        "Did you notice that the `random` value was available to the lambda in `map` even though this takes place on worker nodes while the random singleton is initialized in the driver program? Let us explore this further.\n",
        "\n",
        "First, let us reinitialize our spark context and make it run more local workers."
      ],
      "metadata": {
        "id": "u-oMbhvHbJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"mlibs\").setMaster(\"local[8]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "metadata": {
        "id": "orEOT2UbVqbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a large local list and reference it in the lambda in the map."
      ],
      "metadata": {
        "id": "RlmQmoJoWXu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "slices = 8  # change me\n",
        "list_size = 1000000  # change me\n",
        "times_to_count_len = 50  # change me\n",
        "\n",
        "list1 = list(range(list_size))\n",
        "\n",
        "for i in range(3):\n",
        "   print(f\"Iteration {i}\")\n",
        "   print(\"===========\")\n",
        "   start_time = time.perf_counter()\n",
        "   observed_sizes = sc.parallelize(range(times_to_count_len), slices).map(lambda x: len(list1))\n",
        "   # Collect the small RDD so we can print the observed sizes locally.\n",
        "   print(observed_sizes.glom().collect())\n",
        "   print(f\"Iteration {i} took {time.perf_counter() - start_time} seconds\")\n",
        "   print()"
      ],
      "metadata": {
        "id": "oKQGZt4oKYdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60d764c-746e-48c0-c869-ad3904aaa250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 0 took 2.7856003219999366 seconds\n",
            "\n",
            "Iteration 1\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 1 took 1.8001390440000478 seconds\n",
            "\n",
            "Iteration 2\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 2 took 1.8096045170000252 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the driver program is the bottleneck. The large list has to be transferred from it to all worker nodes. Spark has a mechanism for that called broadcast variables. Compare the running times."
      ],
      "metadata": {
        "id": "bTAvn0akXZ5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "slices = 8  # change me\n",
        "list_size = 1000000  # change me\n",
        "times_to_count_len = 50  # change me\n",
        "\n",
        "list1 = list(range(list_size))\n",
        "\n",
        "for i in range(3):\n",
        "   print(f\"Iteration {i}\")\n",
        "   print(\"===========\")\n",
        "   start_time = time.perf_counter()\n",
        "\n",
        "   # all changes are in this two lines\n",
        "   blist1 = sc.broadcast(list1)\n",
        "   observed_sizes = sc.parallelize(range(times_to_count_len), slices).map(lambda x: len(blist1.value))\n",
        "\n",
        "   # Collect the small RDD so we can print the observed sizes locally.\n",
        "   print(observed_sizes.glom().collect())\n",
        "   print(f\"Iteration {i} took {time.perf_counter() - start_time} seconds\")\n",
        "   print()"
      ],
      "metadata": {
        "id": "rN48017ISUj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cf2ff5-720b-45a3-8873-4deb7093dbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 0 took 1.4530337850000024 seconds\n",
            "\n",
            "Iteration 1\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 1 took 1.252987666000081 seconds\n",
            "\n",
            "Iteration 2\n",
            "===========\n",
            "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
            "Iteration 2 took 1.3458557319999045 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises 1 and 2\n",
        "Implement matrix multiplication with both methods from the lecture. You can use the following sample data to test your Spark solution. For combining two RDDs you can use the `union` transformation. Also use `flatMap`,  `groupByKey`, `aggregateByKey/combineByKey`, `groupByKey`, and `mapValues` for solution 2."
      ],
      "metadata": {
        "id": "CvRWsc93Z3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_sparse(arr):\n",
        "  for r in range(len(arr)):\n",
        "    for c in range(len(arr[0])):\n",
        "      yield ((r,c), arr[r][c])\n",
        "\n",
        "M = [[1,2,3], [4,5,6], [7,8,9]]\n",
        "N = [[1,2], [3,4], [5,6]]\n",
        "MN = np.matmul(M,N)  # [[22,28], [49,64], [76,100]]\n",
        "\n",
        "print(f\"M={list(to_sparse(M))}\")\n",
        "print(f\"N={list(to_sparse(N))}\")\n",
        "print(f\"MN={list(to_sparse(MN))}\")"
      ],
      "metadata": {
        "id": "AV7PGRiqaSkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da38a0c0-3660-40a9-ad72-cb756b25325f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M=[((0, 0), 1), ((0, 1), 2), ((0, 2), 3), ((1, 0), 4), ((1, 1), 5), ((1, 2), 6), ((2, 0), 7), ((2, 1), 8), ((2, 2), 9)]\n",
            "N=[((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4), ((2, 0), 5), ((2, 1), 6)]\n",
            "MN=[((0, 0), 22), ((0, 1), 28), ((1, 0), 49), ((1, 1), 64), ((2, 0), 76), ((2, 1), 100)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "I = len(M)\n",
        "J = len(M[0])  # =len(N)\n",
        "K = len(N[0])\n",
        "\n",
        "Mrdd = sc.parallelize(to_sparse(M))\n",
        "Nrdd = sc.parallelize(to_sparse(N))\n",
        "\n",
        "# put your code here (using solution 1)\n",
        "unionrdd = Mrdd.map(lambda x : (x[0][1], (x[0][0], x[1], \"M\"))).union\\\n",
        "        (Nrdd.map(lambda x : (x[0][0], (x[0][1], x[1], \"N\")))).groupByKey()\n",
        "\n",
        "\n",
        "\n",
        "def solution1(tup):\n",
        "  j, triples = tup\n",
        "  rel_to_tuples = defaultdict(list)\n",
        "\n",
        "  for i_or_k, v, rel in triples:\n",
        "    rel_to_tuples[rel].append((i_or_k,v))\n",
        "  res = []\n",
        "\n",
        "  for i, v in rel_to_tuples[\"M\"]:\n",
        "    for k, w in rel_to_tuples[\"N\"]:\n",
        "      res.append(((i,k), v*w))\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "unionrdd.flatMap(solution1).aggregateByKey(0, lambda x,y : x+y, lambda x,y: x+y).collect()"
      ],
      "metadata": {
        "id": "UcNyNeE6ikLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c38b0c3-58d5-462f-f37a-c4766b7e5f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 1), 64),\n",
              " ((2, 0), 76),\n",
              " ((0, 1), 28),\n",
              " ((0, 0), 22),\n",
              " ((1, 0), 49),\n",
              " ((2, 1), 100)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I = len(M)\n",
        "J = len(M[0])  # =len(N)\n",
        "K = len(N[0])\n",
        "\n",
        "Mrdd = sc.parallelize(to_sparse(M))\n",
        "Nrdd = sc.parallelize(to_sparse(N))\n",
        "\n",
        "Mrdd_multipled = Mrdd.flatMap(lambda t : list( ((t[0][0], k), (t[0][1], t[1]))\\\n",
        "                                              for k in range(K)))\n",
        "Nrdd_multipled = Nrdd.flatMap(lambda t : list( ((i, t[0][1]), (t[0][0], t[1]))\\\n",
        "                                              for i in range(I)))\n",
        "\n",
        "unionrdd = Mrdd_multipled.union(Nrdd_multipled).groupByKey()\n",
        "\n",
        "# put your code here (using solution 2)\n",
        "def solution2(values_with_j):\n",
        "  products_for_j = defaultdict(lambda: 1.0)\n",
        "  for j, v in values_with_j:\n",
        "    products_for_j[j] *= v\n",
        "  return sum(products_for_j.values())\n",
        "\n",
        "unionrdd.mapValues(solution2).collect()"
      ],
      "metadata": {
        "id": "ijNMasM3-A24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b146f347-6083-4f19-be22-a9439575caae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 1), 64.0),\n",
              " ((2, 0), 76.0),\n",
              " ((0, 1), 28.0),\n",
              " ((0, 0), 22.0),\n",
              " ((1, 0), 49.0),\n",
              " ((2, 1), 100.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "Your goal is to join two collections by key. One collection (A) is \"big data\" size and has to be processed on many machines and the other one (B) is small enough to fit in memory of a single machine. Implement the join by using RDD with the first collection and a broadcast variable with the second one."
      ],
      "metadata": {
        "id": "0klBlOWcai_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = [(1,2), (1,3), (2,2)]  # A(X,Y)\n",
        "B = [(2,4), (3,4), (3,5)]  # B(Y,Z)\n",
        "AjoinB = [(1,2,4), (1,3,4), (1,3,5), (2,2,4)]  # A(X,Y) join B(Y,Z) on Y\n",
        "\n",
        "print(f\"A={A}\")\n",
        "print(f\"B={B}\")\n",
        "print(f\"AjoinB={AjoinB}\")"
      ],
      "metadata": {
        "id": "OuuIkdY9nTpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a945dddb-a01e-44f8-c3ce-7e16eabfa797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A=[(1, 2), (1, 3), (2, 2)]\n",
            "B=[(2, 4), (3, 4), (3, 5)]\n",
            "AjoinB=[(1, 2, 4), (1, 3, 4), (1, 3, 5), (2, 2, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "rddA = sc.parallelize(A)\n",
        "\n",
        "B_dict = {}\n",
        "for k, v in B:\n",
        "  B_dict.setdefault(k, []).append(v)\n",
        "\n",
        "broadcast_B_dict = sc.broadcast(B_dict)\n",
        "\n",
        "def solution3(tup):\n",
        "  x, y = tup\n",
        "  vs = broadcast_B_dict.value.get(y, [])\n",
        "  return [(x, y, v) for v in vs]\n",
        "\n",
        "A_join_B = rddA.flatMap(solution3).collect()\n",
        "\n",
        "print(f\"AjoinB={A_join_B}\")"
      ],
      "metadata": {
        "id": "jYfn7HRqnslk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8866dff-8002-487e-e450-7cae1da39fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AjoinB=[(1, 2, 4), (1, 3, 4), (1, 3, 5), (2, 2, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksT9dLxgl9je"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>"
      ]
    }
  ]
}